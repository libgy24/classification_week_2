{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import data \n",
    "\n",
    "products = pd.read_csv('amazon_baby_subset.csv')\n",
    "\n",
    "# fill all the empty reviews with n/a's\n",
    "\n",
    "products = products.fillna({'review':''})\n",
    "\n",
    "# remove all the punctuations.\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None,string.punctuation)\n",
    "\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)\n",
    "\n",
    "# import important words\n",
    "\n",
    "import json\n",
    "\n",
    "with open('important_words.json') as important_words:\n",
    "    important_words = json.load(important_words)\n",
    "with open('module-4-assignment-validation-idx.json') as valid_index:\n",
    "    valid_index = json.load(valid_index)\n",
    "with open('module-4-assignment-train-idx.json') as train_index:\n",
    "    train_index = json.load(train_index)\n",
    "\n",
    "\n",
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda x: x.split().count(word))\n",
    "    \n",
    "# split data into training and validtion\n",
    "\n",
    "train_data = products.loc[train_index]\n",
    "\n",
    "validation_data = products.loc[valid_index]\n",
    "\n",
    "# convert data frame into matrix\n",
    "\n",
    "def get_numpy_data(dataframe,features,target):\n",
    "    dataframe['constant'] = 1\n",
    "    features = ['constant'] + features\n",
    "    feature_matrix = dataframe[features].values\n",
    "    target_array = dataframe[target].values\n",
    "    return feature_matrix,target_array\n",
    "\n",
    "feature_matrix_train, sentiment_train = get_numpy_data(train_data, important_words, 'sentiment')\n",
    "feature_matrix_valid, sentiment_valid = get_numpy_data(validation_data, important_words, 'sentiment') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write a function to produce probablistic estimate for p(y_i = +1|x,w)\n",
    "\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    scores = feature_matrix.dot(coefficients)\n",
    "    predictions = 1/(1 + np.exp(np.negative(scores)))\n",
    "    return predictions\n",
    "\n",
    "# write a function to compute the derivative\n",
    "\n",
    "def feature_derivative_with_l2(errors, features, coefficient, l2_penalty,feature_is_constant):\n",
    "    derivative = features.dot(errors)\n",
    "    \n",
    "    if not feature_is_constant:\n",
    "        derivative = derivative - 2*l2_penalty*coefficient\n",
    "    return derivative\n",
    "            \n",
    "# write a function to compute log-likelihood\n",
    "\n",
    "def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores))) - l2_penalty*np.sum(coefficients[1:]**2)\n",
    "    \n",
    "    return lp\n",
    "\n",
    "# write a function logistic_regression_with_l2 to fit a logistic regression moel under L2 regularization.\n",
    "\n",
    "def logistic_regression_with_l2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter):\n",
    "    coefficients = np.array(initial_coefficients)\n",
    "    for itr in xrange(max_iter):\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        indicator = (sentiment == +1)\n",
    "        errors = indicator - predictions\n",
    "        for j in xrange(len(coefficients)):\n",
    "            is_intercept = (j == 0)\n",
    "            derivative = feature_matrix[:,j].T.dot(errors)\n",
    "            if not is_intercept:\n",
    "                derivative = derivative - 2 * l2_penalty*coefficients[j]\n",
    "            coefficients[j] = coefficients[j] + step_size * derivative\n",
    "            \n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 ==0) or\\\n",
    "        (itr <= 10000 and itr % 1000 == 0) or itr % 10000 ==0:\n",
    "            lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n",
    "            print 'iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp)\n",
    "    return coefficients\n",
    "\n",
    "def sorted_coefficients(coefficients, reverse_value):\n",
    "    coefficients = list(coefficients[1:])\n",
    "\n",
    "    word_coefficients_tuples = [(word, coefficient) for word,coefficient in zip(important_words, coefficients)]\n",
    "\n",
    "    word_coefficients_tuples = sorted(word_coefficients_tuples, key = lambda x:x[1], reverse = reverse_value)\n",
    "    return word_coefficients_tuples\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_matrix = feature_matrix_train\n",
    "sentiment = sentiment_train\n",
    "initial_coefficients = np.zeros(194)\n",
    "step_size = 5e-6\n",
    "max_iter = 501\n",
    "l2_0_penalty = 0.0\n",
    "l2_4_penalty = 4.0\n",
    "l2_10_penalty = 10.0\n",
    "l2_1e2_penalty = 1e2\n",
    "l2_1e3_penalty = 1e3\n",
    "l2_1e5_penalty = 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.39138303\n",
      "iteration   1: log likelihood of observed labels = -29003.71259047\n",
      "iteration   2: log likelihood of observed labels = -28834.66187288\n",
      "iteration   3: log likelihood of observed labels = -28671.70781507\n",
      "iteration   4: log likelihood of observed labels = -28514.43078198\n",
      "iteration   5: log likelihood of observed labels = -28362.48344665\n",
      "iteration   6: log likelihood of observed labels = -28215.56713122\n",
      "iteration   7: log likelihood of observed labels = -28073.41743783\n",
      "iteration   8: log likelihood of observed labels = -27935.79536396\n",
      "iteration   9: log likelihood of observed labels = -27802.48168669\n",
      "iteration  10: log likelihood of observed labels = -27673.27331484\n",
      "iteration  11: log likelihood of observed labels = -27547.98083656\n",
      "iteration  12: log likelihood of observed labels = -27426.42679977\n",
      "iteration  13: log likelihood of observed labels = -27308.44444728\n",
      "iteration  14: log likelihood of observed labels = -27193.87673876\n",
      "iteration  15: log likelihood of observed labels = -27082.57555831\n",
      "iteration  20: log likelihood of observed labels = -26570.43059938\n",
      "iteration  30: log likelihood of observed labels = -25725.48742389\n",
      "iteration  40: log likelihood of observed labels = -25055.53326910\n",
      "iteration  50: log likelihood of observed labels = -24509.63590026\n",
      "iteration  60: log likelihood of observed labels = -24054.97906083\n",
      "iteration  70: log likelihood of observed labels = -23669.51640848\n",
      "iteration  80: log likelihood of observed labels = -23337.89167628\n",
      "iteration  90: log likelihood of observed labels = -23049.07066021\n",
      "iteration 100: log likelihood of observed labels = -22794.90974921\n",
      "iteration 200: log likelihood of observed labels = -21283.29527353\n",
      "iteration 300: log likelihood of observed labels = -20570.97485473\n",
      "iteration 400: log likelihood of observed labels = -20152.21466944\n",
      "iteration 500: log likelihood of observed labels = -19876.62333410\n"
     ]
    }
   ],
   "source": [
    "coefficients_0_penalty = logistic_regression_with_l2(feature_matrix, sentiment, initial_coefficients, step_size, \\\n",
    "                                                     l2_0_penalty, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.39508175\n",
      "iteration   1: log likelihood of observed labels = -29003.73417180\n",
      "iteration   2: log likelihood of observed labels = -28834.71441858\n",
      "iteration   3: log likelihood of observed labels = -28671.80345068\n",
      "iteration   4: log likelihood of observed labels = -28514.58077957\n",
      "iteration   5: log likelihood of observed labels = -28362.69830317\n",
      "iteration   6: log likelihood of observed labels = -28215.85663259\n",
      "iteration   7: log likelihood of observed labels = -28073.79071393\n",
      "iteration   8: log likelihood of observed labels = -27936.26093762\n",
      "iteration   9: log likelihood of observed labels = -27803.04751805\n",
      "iteration  10: log likelihood of observed labels = -27673.94684207\n",
      "iteration  11: log likelihood of observed labels = -27548.76901327\n",
      "iteration  12: log likelihood of observed labels = -27427.33612958\n",
      "iteration  13: log likelihood of observed labels = -27309.48101569\n",
      "iteration  14: log likelihood of observed labels = -27195.04624253\n",
      "iteration  15: log likelihood of observed labels = -27083.88333261\n",
      "iteration  20: log likelihood of observed labels = -26572.49874392\n",
      "iteration  30: log likelihood of observed labels = -25729.32604153\n",
      "iteration  40: log likelihood of observed labels = -25061.34245801\n",
      "iteration  50: log likelihood of observed labels = -24517.52091982\n",
      "iteration  60: log likelihood of observed labels = -24064.99093939\n",
      "iteration  70: log likelihood of observed labels = -23681.67373669\n",
      "iteration  80: log likelihood of observed labels = -23352.19298741\n",
      "iteration  90: log likelihood of observed labels = -23065.50180166\n",
      "iteration 100: log likelihood of observed labels = -22813.44844580\n",
      "iteration 200: log likelihood of observed labels = -21321.14164794\n",
      "iteration 300: log likelihood of observed labels = -20624.98634439\n",
      "iteration 400: log likelihood of observed labels = -20219.92048845\n",
      "iteration 500: log likelihood of observed labels = -19956.11341777\n"
     ]
    }
   ],
   "source": [
    "coefficients_4_penalty = logistic_regression_with_l2(feature_matrix, sentiment, initial_coefficients, step_size, \\\n",
    "                                                     l2_4_penalty, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.40062984\n",
      "iteration   1: log likelihood of observed labels = -29003.76654163\n",
      "iteration   2: log likelihood of observed labels = -28834.79322654\n",
      "iteration   3: log likelihood of observed labels = -28671.94687528\n",
      "iteration   4: log likelihood of observed labels = -28514.80571589\n",
      "iteration   5: log likelihood of observed labels = -28363.02048079\n",
      "iteration   6: log likelihood of observed labels = -28216.29071186\n",
      "iteration   7: log likelihood of observed labels = -28074.35036891\n",
      "iteration   8: log likelihood of observed labels = -27936.95892966\n",
      "iteration   9: log likelihood of observed labels = -27803.89576265\n",
      "iteration  10: log likelihood of observed labels = -27674.95647005\n",
      "iteration  11: log likelihood of observed labels = -27549.95042714\n",
      "iteration  12: log likelihood of observed labels = -27428.69905549\n",
      "iteration  13: log likelihood of observed labels = -27311.03455140\n",
      "iteration  14: log likelihood of observed labels = -27196.79890162\n",
      "iteration  15: log likelihood of observed labels = -27085.84308528\n",
      "iteration  20: log likelihood of observed labels = -26575.59697506\n",
      "iteration  30: log likelihood of observed labels = -25735.07304608\n",
      "iteration  40: log likelihood of observed labels = -25070.03447306\n",
      "iteration  50: log likelihood of observed labels = -24529.31188025\n",
      "iteration  60: log likelihood of observed labels = -24079.95349572\n",
      "iteration  70: log likelihood of observed labels = -23699.83199186\n",
      "iteration  80: log likelihood of observed labels = -23373.54108747\n",
      "iteration  90: log likelihood of observed labels = -23090.01500055\n",
      "iteration 100: log likelihood of observed labels = -22841.08995135\n",
      "iteration 200: log likelihood of observed labels = -21377.25595328\n",
      "iteration 300: log likelihood of observed labels = -20704.63995428\n",
      "iteration 400: log likelihood of observed labels = -20319.25685307\n",
      "iteration 500: log likelihood of observed labels = -20072.16321721\n"
     ]
    }
   ],
   "source": [
    "coefficients_10_penalty = logistic_regression_with_l2(feature_matrix, sentiment, initial_coefficients, step_size, \\\n",
    "                                                     l2_10_penalty, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.48385120\n",
      "iteration   1: log likelihood of observed labels = -29004.25177457\n",
      "iteration   2: log likelihood of observed labels = -28835.97382190\n",
      "iteration   3: log likelihood of observed labels = -28674.09410083\n",
      "iteration   4: log likelihood of observed labels = -28518.17112932\n",
      "iteration   5: log likelihood of observed labels = -28367.83774654\n",
      "iteration   6: log likelihood of observed labels = -28222.77708939\n",
      "iteration   7: log likelihood of observed labels = -28082.70799392\n",
      "iteration   8: log likelihood of observed labels = -27947.37595368\n",
      "iteration   9: log likelihood of observed labels = -27816.54738615\n",
      "iteration  10: log likelihood of observed labels = -27690.00588850\n",
      "iteration  11: log likelihood of observed labels = -27567.54970126\n",
      "iteration  12: log likelihood of observed labels = -27448.98991327\n",
      "iteration  13: log likelihood of observed labels = -27334.14912742\n",
      "iteration  14: log likelihood of observed labels = -27222.86041863\n",
      "iteration  15: log likelihood of observed labels = -27114.96648229\n",
      "iteration  20: log likelihood of observed labels = -26621.50201299\n",
      "iteration  30: log likelihood of observed labels = -25819.72803950\n",
      "iteration  40: log likelihood of observed labels = -25197.34035501\n",
      "iteration  50: log likelihood of observed labels = -24701.03698195\n",
      "iteration  60: log likelihood of observed labels = -24296.66378580\n",
      "iteration  70: log likelihood of observed labels = -23961.38842316\n",
      "iteration  80: log likelihood of observed labels = -23679.38088853\n",
      "iteration  90: log likelihood of observed labels = -23439.31824267\n",
      "iteration 100: log likelihood of observed labels = -23232.88192018\n",
      "iteration 200: log likelihood of observed labels = -22133.50726528\n",
      "iteration 300: log likelihood of observed labels = -21730.03957488\n",
      "iteration 400: log likelihood of observed labels = -21545.87572145\n",
      "iteration 500: log likelihood of observed labels = -21451.95551390\n"
     ]
    }
   ],
   "source": [
    "coefficients_1e2_penalty = logistic_regression_with_l2(feature_matrix, sentiment, initial_coefficients, step_size, \\\n",
    "                                                     l2_1e2_penalty, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29180.31606471\n",
      "iteration   1: log likelihood of observed labels = -29009.07176112\n",
      "iteration   2: log likelihood of observed labels = -28847.62378912\n",
      "iteration   3: log likelihood of observed labels = -28695.14439397\n",
      "iteration   4: log likelihood of observed labels = -28550.95060743\n",
      "iteration   5: log likelihood of observed labels = -28414.45771129\n",
      "iteration   6: log likelihood of observed labels = -28285.15124375\n",
      "iteration   7: log likelihood of observed labels = -28162.56976044\n",
      "iteration   8: log likelihood of observed labels = -28046.29387744\n",
      "iteration   9: log likelihood of observed labels = -27935.93902900\n",
      "iteration  10: log likelihood of observed labels = -27831.15045502\n",
      "iteration  11: log likelihood of observed labels = -27731.59955260\n",
      "iteration  12: log likelihood of observed labels = -27636.98108219\n",
      "iteration  13: log likelihood of observed labels = -27547.01092670\n",
      "iteration  14: log likelihood of observed labels = -27461.42422295\n",
      "iteration  15: log likelihood of observed labels = -27379.97375625\n",
      "iteration  20: log likelihood of observed labels = -27027.18208317\n",
      "iteration  30: log likelihood of observed labels = -26527.22737267\n",
      "iteration  40: log likelihood of observed labels = -26206.59048765\n",
      "iteration  50: log likelihood of observed labels = -25995.96903148\n",
      "iteration  60: log likelihood of observed labels = -25854.95710284\n",
      "iteration  70: log likelihood of observed labels = -25759.08109950\n",
      "iteration  80: log likelihood of observed labels = -25693.05688014\n",
      "iteration  90: log likelihood of observed labels = -25647.09929349\n",
      "iteration 100: log likelihood of observed labels = -25614.81468705\n",
      "iteration 200: log likelihood of observed labels = -25536.20998919\n",
      "iteration 300: log likelihood of observed labels = -25532.57691220\n",
      "iteration 400: log likelihood of observed labels = -25532.35543765\n",
      "iteration 500: log likelihood of observed labels = -25532.33970049\n"
     ]
    }
   ],
   "source": [
    "coefficients_1e3_penalty = logistic_regression_with_l2(feature_matrix, sentiment, initial_coefficients, step_size, \\\n",
    "                                                     l2_1e3_penalty, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29271.85955115\n",
      "iteration   1: log likelihood of observed labels = -29271.71006589\n",
      "iteration   2: log likelihood of observed labels = -29271.65738833\n",
      "iteration   3: log likelihood of observed labels = -29271.61189923\n",
      "iteration   4: log likelihood of observed labels = -29271.57079975\n",
      "iteration   5: log likelihood of observed labels = -29271.53358505\n",
      "iteration   6: log likelihood of observed labels = -29271.49988440\n",
      "iteration   7: log likelihood of observed labels = -29271.46936584\n",
      "iteration   8: log likelihood of observed labels = -29271.44172890\n",
      "iteration   9: log likelihood of observed labels = -29271.41670149\n",
      "iteration  10: log likelihood of observed labels = -29271.39403722\n",
      "iteration  11: log likelihood of observed labels = -29271.37351294\n",
      "iteration  12: log likelihood of observed labels = -29271.35492661\n",
      "iteration  13: log likelihood of observed labels = -29271.33809523\n",
      "iteration  14: log likelihood of observed labels = -29271.32285309\n",
      "iteration  15: log likelihood of observed labels = -29271.30905015\n",
      "iteration  20: log likelihood of observed labels = -29271.25729150\n",
      "iteration  30: log likelihood of observed labels = -29271.20657205\n",
      "iteration  40: log likelihood of observed labels = -29271.18775997\n",
      "iteration  50: log likelihood of observed labels = -29271.18078247\n",
      "iteration  60: log likelihood of observed labels = -29271.17819447\n",
      "iteration  70: log likelihood of observed labels = -29271.17723457\n",
      "iteration  80: log likelihood of observed labels = -29271.17687853\n",
      "iteration  90: log likelihood of observed labels = -29271.17674648\n",
      "iteration 100: log likelihood of observed labels = -29271.17669750\n",
      "iteration 200: log likelihood of observed labels = -29271.17666862\n",
      "iteration 300: log likelihood of observed labels = -29271.17666862\n",
      "iteration 400: log likelihood of observed labels = -29271.17666862\n",
      "iteration 500: log likelihood of observed labels = -29271.17666862\n"
     ]
    }
   ],
   "source": [
    "coefficients_1e5_penalty = logistic_regression_with_l2(feature_matrix, sentiment, initial_coefficients, step_size, \\\n",
    "                                                     l2_1e5_penalty, max_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
